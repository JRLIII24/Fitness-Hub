# Exercise Data Import Scripts

This directory contains scripts to import 11,000+ exercises from open source datasets into the Supabase database.

## Data Sources

### 1. ExerciseDB (Primary Source)
- **Repository**: https://github.com/ExerciseDB/exercisedb-api
- **Exercises**: 11,000+
- **Features**: GIFs, videos, detailed instructions, equipment, target muscles
- **License**: Open source (MIT)

### 2. Free Exercise DB (Secondary Source)
- **Repository**: https://github.com/yuhonas/free-exercise-db
- **Exercises**: 800+
- **Features**: Images, instructions, muscle groups, equipment
- **License**: Public domain

## Prerequisites

1. **Environment Variables**: Add to `.env.local`:
```bash
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key  # Required for import script
```

2. **Run Migration**: Execute migration 028 in Supabase dashboard SQL editor:
```bash
# Copy contents of supabase/migrations/028_exercise_api_integration.sql
# Run in Supabase Dashboard ‚Üí SQL Editor
```

## Running the Import

### Step 1: Install Dependencies
```bash
pnpm install
```

### Step 2: Run Import Script
```bash
pnpm run import:exercises
```

The script will:
1. Download ExerciseDB dataset from GitHub (~11,000 exercises)
2. Download Free Exercise DB dataset from GitHub (~800 exercises)
3. Normalize both datasets to internal schema
4. Deduplicate exercises (prefer ExerciseDB for exercises in both datasets)
5. Validate exercise data (filter out invalid entries)
6. Batch import to Supabase (100 exercises per batch)
7. Display summary statistics

### Expected Output
```
üèãÔ∏è  Starting Exercise Data Import...

Step 1: Fetching ExerciseDB data from GitHub...
‚úÖ Fetched 11,000 exercises from ExerciseDB

Step 2: Fetching Free Exercise DB data from GitHub...
‚úÖ Fetched 800 exercises from Free Exercise DB

Step 3: Normalizing exercise data...
‚úÖ Normalized 11,800 exercises

Step 4: Deduplicating exercises...
‚úÖ Deduplicated to 11,300 unique exercises

Step 5: Validating exercise data...
‚úÖ 10,800 exercises passed validation (500 filtered out)

Step 6: Importing exercises to Supabase...
‚úÖ Successfully imported 10,800 exercises to database

üìä Import Summary:
  - ExerciseDB:        11,000 exercises
  - Free Exercise DB:  800 exercises
  - After deduplication: 11,300 exercises
  - After validation:  10,800 exercises
  - Inserted to DB:    10,800 exercises

üìà Database Counts:
  - ExerciseDB:        10,500 exercises
  - Free Exercise DB:  300 exercises

‚úÖ Import complete!
```

## Data Normalization

### Muscle Group Mapping
ExerciseDB and Free Exercise DB use different muscle group naming conventions. The script maps them to our internal schema:

**Our Schema**: `chest`, `back`, `legs`, `shoulders`, `arms`, `core`, `cardio`

**ExerciseDB Mapping**:
- `chest` ‚Üí `chest`
- `back` ‚Üí `back`
- `upper legs`, `lower legs` ‚Üí `legs`
- `shoulders` ‚Üí `shoulders`
- `upper arms`, `lower arms` ‚Üí `arms`
- `waist` ‚Üí `core`
- `cardio` ‚Üí `cardio`

**Free Exercise DB Mapping**:
- `biceps`, `triceps`, `forearms` ‚Üí `arms`
- `quadriceps`, `hamstrings`, `calves`, `glutes` ‚Üí `legs`
- `abdominals` ‚Üí `core`
- etc.

### Equipment Mapping
Both datasets are normalized to our equipment categories:
- `barbell`
- `dumbbell`
- `cable`
- `machine`
- `bodyweight`
- `band`
- `kettlebell`
- `other`

### Category Inference
The script automatically categorizes exercises based on name and characteristics:
- **Compound**: Multi-joint movements (squat, deadlift, bench press, etc.)
- **Isolation**: Single-joint movements (curls, extensions, etc.)
- **Cardio**: Running, jumping, cycling, etc.
- **Mobility**: Stretching, foam rolling, etc.

## Deduplication Strategy

When exercises exist in both datasets with the same name and equipment:
1. **Prefer ExerciseDB** (has GIFs + better data quality)
2. **Fallback to Free Exercise DB** if not in ExerciseDB
3. **Keep both** if name or equipment differs

## Database Schema

The import script populates the `exercises` table with these additional columns (from migration 028):

```sql
source TEXT                 -- 'exercisedb', 'free-exercise-db', 'library', or 'custom'
source_exercise_id TEXT     -- Original ID from external source
gif_url TEXT                -- URL to GIF demonstration (ExerciseDB only)
```

Existing columns:
```sql
id UUID                     -- Auto-generated by Supabase
name TEXT                   -- Exercise name
slug TEXT                   -- URL-safe name
muscle_group TEXT           -- Primary muscle targeted
equipment TEXT              -- Required equipment (null = bodyweight)
category TEXT               -- compound, isolation, cardio, or mobility
instructions TEXT           -- Step-by-step instructions
form_tips TEXT[]            -- Safety and form tips
image_url TEXT              -- Static image (or GIF URL for ExerciseDB)
```

## Re-running the Import

The import script uses `UPSERT` with `onConflict: "source,source_exercise_id"`, so:
- ‚úÖ Safe to re-run (won't create duplicates)
- ‚úÖ Updates existing exercises with newer data
- ‚úÖ Preserves user-created custom exercises (`source = 'custom'`)

## Troubleshooting

### Error: "Cannot find project ref"
- Run migration 028 manually in Supabase dashboard instead of using CLI
- Make sure `.env.local` has correct Supabase credentials

### Error: "SUPABASE_SERVICE_ROLE_KEY is missing"
- Get service role key from Supabase Dashboard ‚Üí Settings ‚Üí API
- Add to `.env.local`
- **Warning**: Never commit service role key to git (it's in `.gitignore`)

### Import fails partway through
- Script uses batch insertion (100 per batch)
- Check Supabase logs for specific error
- Re-run script (it will skip already imported exercises)

### Fewer exercises imported than expected
- Some exercises filtered out during validation (missing required fields)
- Check console output for "X exercises passed validation" message
- This is normal - not all exercises from external sources have complete data

## Cost Analysis

**Total Cost**: $0/month FOREVER ‚úÖ

- ExerciseDB: Open source, self-hosted dataset
- Free Exercise DB: Public domain dataset
- No API calls, no rate limits, no ongoing costs
- One-time import, unlimited searches

## Next Steps

After import is complete:
1. Create exercise search API route (`src/app/api/exercises/search/route.ts`)
2. Update workout page exercise picker to use new search
3. Add GIF preview support for ExerciseDB exercises
4. Test search performance (should be <100ms with full-text search indexes)
